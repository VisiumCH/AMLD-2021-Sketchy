{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook enables to predict the closest images of a sketch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.loader_factory import load_data\n",
    "from src.models.encoder import EncoderCNN\n",
    "from src.models.utils import load_checkpoint\n",
    "from src.models.test import get_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dataset = \"sketchy_extend\"\n",
    "    data_path = \"../io/data/raw\"\n",
    "    emb_size = 256\n",
    "    grl_lambda = 0.5\n",
    "    nopretrain = False\n",
    "    epochs = 1000\n",
    "    batch_size = 10\n",
    "    seed = 42\n",
    "    load = None\n",
    "    early_stop = 20\n",
    "    ngpu = 1\n",
    "    prefetch = 2\n",
    "    log = \"../io/models/\"\n",
    "    log_interval = 20\n",
    "    attn = True\n",
    "    plot = False\n",
    "    cuda = True\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CHECKPOINT = '../io/models/1_run-batch_size_10/checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args, best_checkpoint):\n",
    "    im_net = EncoderCNN(out_size=args.emb_size, attention=True)\n",
    "    sk_net = EncoderCNN(out_size=args.emb_size, attention=True)\n",
    "    \n",
    "    checkpoint = load_checkpoint(best_checkpoint)\n",
    "    im_net.load_state_dict(checkpoint['im_state'])\n",
    "    sk_net.load_state_dict(checkpoint['sk_state'])\n",
    "    return im_net, sk_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading model '../io/models/1_run-batch_size_10/checkpoint.pth'\n",
      "=> loaded model '../io/models/1_run-batch_size_10/checkpoint.pth' (epoch 42, map 0.2796034388821055)\n"
     ]
    }
   ],
   "source": [
    "im_net, sk_net = get_model(args, BEST_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all images and compute their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "_, [_, _], [test_sk_data, test_im_data], dict_class = load_data(args, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Sketch: 12694\n",
      "Length Image: 10453\n",
      "Classes: ['bat', 'cabin', 'cow', 'dolphin', 'door', 'giraffe', 'helicopter', 'mouse', 'pear', 'raccoon', 'rhinoceros', 'saw', 'scissors', 'seagull', 'skyscraper', 'songbird', 'sword', 'tree', 'wheelchair', 'windmill', 'window']\n",
      "Num Classes: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Length Sketch: {}\".format(len(test_sk_data)))\n",
    "print(\"Length Image: {}\".format(len(test_im_data)))\n",
    "print(\"Classes: {}\".format(test_sk_data.get_class_dict()))\n",
    "print(\"Num Classes: {}\".format(len(test_sk_data.get_class_dict())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im_loader = DataLoader(test_im_data, batch_size=3 * args.batch_size, num_workers=args.prefetch, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6f6fc6ff10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ea1569b6ad7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_im_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AMLD-2021-Sketchy/src/models/test.py\u001b[0m in \u001b[0;36mget_test_data\u001b[0;34m(data_loader, model, args)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Data to Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "fnames, embeddings, classes = get_test_data(test_im_loader, im_net, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([30, 3, 224, 224])\n",
      "torch.Size([30])\n",
      "['../io/data/raw/Sketchy/extended_photo/bat/ext_274.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_137.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02142407_538.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_98.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02139199_9033.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_305.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02139199_14100.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_199.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_183.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02139199_15837.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02149420_28.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_202.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02139199_16702.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_204.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02139199_14279.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_56.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_140.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_101.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_211.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_246.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_170.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_8.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_168.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_226.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_130.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_98.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_80.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_250.jpg', '../io/data/raw/Sketchy/extended_photo/bat/n02139199_8901.jpg', '../io/data/raw/Sketchy/extended_photo/bat/ext_287.jpg']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e6dcf451c2fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "fnames = []\n",
    "for i, (image, fname, target) in enumerate(test_im_loader):\n",
    "    print(i)\n",
    "    print(image.shape)\n",
    "    print(fname.shape)\n",
    "    print(target)\n",
    "\n",
    "    # Process\n",
    "    out_features, _ = im_net(image)\n",
    "\n",
    "    # Filename of the images for qualitative\n",
    "    fnames.append(fname)\n",
    "\n",
    "    if i == 0:\n",
    "        embeddings = out_features.cpu().data.numpy()\n",
    "        classes = target.cpu().data.numpy()\n",
    "    else:\n",
    "        embeddings = np.concatenate((embeddings, out_features.cpu().data.numpy()), axis=0)\n",
    "        classes = np.concatenate((classes, target.cpu().data.numpy()), axis=0)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sketch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find closest images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMLD env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
