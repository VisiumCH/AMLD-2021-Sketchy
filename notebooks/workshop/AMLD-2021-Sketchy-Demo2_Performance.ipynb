{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLD-2021-Sketchy: Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we visualised the datasets, coded the model encoder and coded the losses. In this notebook, we compute and visualise the perfomance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.data.loader_factory import load_data\n",
    "from src.data.utils import default_image_loader\n",
    "from src.models.encoder import EncoderCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare here the data on which we will compute the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLOSEST = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dataset = \"sketchy\"\n",
    "    data_path = 'io/data/raw/'\n",
    "    save = 'io/models/sktu_copy/'\n",
    "    load = save + 'checkpoint.pth'\n",
    "    embeddings_path = save + '00053/default/'\n",
    "    emb_size = 256\n",
    "    cuda = False\n",
    "    max_images_test = 10\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "# Note: the code here is simplified, only choose sketchy or quickdraw as dataset.\n",
    "assert args.dataset in ['sketchy', 'quickdraw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use the train and test data for this notebook\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data, [_, _], [test_sk_data, test_im_data], dict_class_to_id = load_data(args, transform)\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n",
    "                          num_workers=args.prefetch, pin_memory=args.cuda, drop_last=True)\n",
    "\n",
    "# We will work on a single batch\n",
    "sketchs, positive_images, negative_images, positive_labels, negative_labels = next(iter(train_loader))\n",
    "positive_labels, negative_labels = positive_labels.numpy(), negative_labels.numpy()\n",
    "\n",
    "# Reverse dict to go from id to class\n",
    "dict_id_to_class = {v:k for k,v in dict_class_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model\n",
    "im_net = EncoderCNN(out_size=args.emb_size, attention=args.attn)\n",
    "sk_net = EncoderCNN(out_size=args.emb_size, attention=args.attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sketch\n",
    "sketch_fname = '../../io/data/raw/Quickdraw/sketches/ant/4531454526619648.png'\n",
    "sketch = default_image_loader(sketch_fname) # basically load and resize the image\n",
    "sketch = transform(sketch) # apply transform\n",
    "sketch = sketch.unsqueeze(0) # expand because no batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_net.eval()\n",
    "sk_net.eval()\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, to performe the inference, one first need to compute the embeddings for all the images. As this is a long and quite straightforward process, we already have done it for you and saved them in a .npy file. Here is the function to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precomputed_embeddings(args, dataset_type):\n",
    "    \n",
    "    assert dataset_type in [\"test\", \"valid\"]\n",
    "\n",
    "    precomputed_embeddings_path = os.path.join(args.save, \"precomputed_embeddings\")\n",
    "    embeddings_path = os.path.join(precomputed_embeddings_path, f\"{args.dataset}_{dataset_type}_array.npy\")\n",
    "    meta_path = os.path.join(precomputed_embeddings_path, f\"{args.dataset}_{dataset_type}_meta.csv\")\n",
    "\n",
    "    if not os.path.exists(embeddings_path):\n",
    "        raise FileNotFoundError(\"The embeddings have not been precomputed for this dataset yet.\")\n",
    "\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        images_embeddings = np.load(f)\n",
    "    df = pd.read_csv(meta_path, sep=\" \")\n",
    "    \n",
    "    images_fnames, images_classes = df[\"fnames\"].to_list(), df[\"classes\"].to_list()\n",
    "    \n",
    "    return images_embeddings, images_classes, images_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_embeddings, images_classes, images_fnames = load_precomputed_embeddings(args, dataset_type=\"test\")\n",
    "print(images_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute these embeddings for the sketches as well. Here is the function that we used before to compute the embeddings. It takes a data loader and a model as parameter and returns the embeddings, along with the path and class of each image/sketch. You can also add the parameters max_num_batches and num_first_batch in the args, which will respectively stop the iteration when a maximum number of batch has been seen and skip the first batches while looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(data_loader, model, args):\n",
    "\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total = args.__dict__.get(\"max_num_batches\", len(data_loader))\n",
    "    num_first_batch = args.__dict__.get(\"num_first_batch\", 0)\n",
    "    for i, (image, fname, target) in enumerate(tqdm(data_loader, total=total)):\n",
    "\n",
    "        # Ignore first batches and stop when reaching max number\n",
    "        if i < num_first_batch:\n",
    "            continue\n",
    "        if i >= total + num_first_batch :\n",
    "            break\n",
    "\n",
    "        # Data to Variable\n",
    "        if args.cuda:\n",
    "            image, target = image.cuda(), target.cuda()\n",
    "\n",
    "        # Process\n",
    "        out_features, _ = model(image)\n",
    "\n",
    "        if args.cuda:\n",
    "            out_features = out_features.cpu().data.numpy()\n",
    "            target = target.cpu().data.numpy()\n",
    "        else:\n",
    "            out_features = out_features.detach().numpy()\n",
    "\n",
    "        if i == num_first_batch:\n",
    "            fnames = [fname]\n",
    "            embeddings = out_features\n",
    "            classes = target\n",
    "        else:\n",
    "            fnames.append(fnames)\n",
    "            embeddings = np.concatenate((embeddings, out_features), axis=0)\n",
    "            classes = np.concatenate((classes, target), axis=0)\n",
    "            \n",
    "    # Save\n",
    "    ...\n",
    "\n",
    "    return embeddings, classes, fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is quite slow, we only compute 300 sketches, which should be enough to assess our model performance. Feel free to increase this number (as long as your memory holds ðŸ˜‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute 10*30=300 sketches embeddings\n",
    "args.batch_size_test = 30\n",
    "args.max_num_batches = 10\n",
    "args.num_first_batch  = 0\n",
    "\n",
    "# Get the DataLoader\n",
    "sk_test_loader = DataLoader(test_sk_data, batch_size=args.batch_size_test,\n",
    "                       num_workers=args.prefetch, pin_memory=True)\n",
    "\n",
    "\n",
    "sketch_test_embeddings, sketch_test_classes, sketch_test_fnames = compute_embeddings(sk_test_loader, sk_net, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sketch_test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both the embeddings for the images and the sketches, we can perform the inference by computing the similarity between them in the embeddings space. we define the similarity by the following simple function (with the distance being the Euclidian distance between both embeddings): $$similarity = \\frac{1}{1 + distance}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(im_embeddings, sk_embeddings):\n",
    "    '''\n",
    "    Computes images and sketch similarity in the feature space.\n",
    "    The distance is computed as the euclidean distance. \n",
    "    Here, we want similarity = 1/(1 + distance)\n",
    "    Args:\n",
    "        - im_embeddings: embeddings of the images [MxE]\n",
    "        - sk_embeddings: embeddings of the sketches [NxE]\n",
    "    Return:\n",
    "        - similarity: similarity value between images and sketches embeddings [NxM]\n",
    "    '''\n",
    "    \n",
    "    similarity = ... # Implement me !\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def get_similarity_solution(im_embeddings, sk_embeddings):\n",
    "\n",
    "    from scipy.spatial.distance import cdist\n",
    "    \n",
    "    similarity = 1/(1 + cdist(sk_embeddings, im_embeddings, 'euclidean'))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = get_similarity_solution(images_embeddings, sketch_test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the labels of the loaded sketches and embeddings. 1 where the label and sketch match and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classes(class_im, class_sk):\n",
    "    '''\n",
    "    Compare classes of images and sketches\n",
    "    Args:\n",
    "        - class_im: list of classes of the images [M]\n",
    "        - class_sk: list of classes of the sketches [N]\n",
    "    Return:\n",
    "        - array [MxN] of 1 where the image and sketch belong to the same class and 0 elsewhere\n",
    "    '''\n",
    "    class_sk = np.expand_dims(class_sk, axis=1)\n",
    "    class_im = np.expand_dims(class_im, axis=0)\n",
    "    class_matches = ... # Implement me!\n",
    "    return class_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classes_solution(class_im, class_sk):\n",
    "    class_sk = np.expand_dims(class_sk, axis=1)\n",
    "    class_im = np.expand_dims(class_im, axis=0)\n",
    "    class_matches = (class_sk == class_im)*1\n",
    "    return class_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_matches = compare_classes_solution(images_classes, sketch_test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the classes by similarity meaning that for each sketch, we get a list of the closest images' classes based on theis similarity values. An image similar to a sketch should be in the beginning of the least and a very differnt image should be at the end of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_class_by_similarity(similarity, class_matches):\n",
    "    similarity.squeeze()\n",
    "    arg_sorted_sim = (-similarity).argsort()\n",
    "    sorted_similarity = []  # list of similarity values ordered by similarity (most to least similar)\n",
    "    sorted_class = []       # list of class match ordered by similarity (0 if different, 1 if same)\n",
    "    for index in range(0, arg_sorted_sim.shape[0]):\n",
    "        sorted_similarity.append(similarity[index, arg_sorted_sim[index, :]])\n",
    "        sorted_class.append(class_matches[index, arg_sorted_sim[index, :]])\n",
    "\n",
    "    sorted_similarity = np.array(sorted_similarity)\n",
    "    sorted_class_matches = np.array(sorted_class)\n",
    "    return sorted_similarity, sorted_class_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_similarity, sorted_class_matches = sort_class_by_similarity(similarity, class_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Compute the mAP (mean average precision) and Precision@100 of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean average precision of all results\n",
    "# Hint: you can use sklearn function average_precision_score\n",
    "limit = 100\n",
    "map_ = ... # Implement me!\n",
    "\n",
    "# Compute the mean average precision of the 100 first results\n",
    "map_100 = ... # Implement me!\n",
    "\n",
    "# Compute the Precision@100 (meaning the precision at the place 100th)\n",
    "prec_100 =... # Implement me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "limit = 100\n",
    "\n",
    "map_ = average_precision_score(sorted_class_matches, sorted_similarity)\n",
    "print(f'The mean average precision of all samples is {np.round(map_, 4)}.')\n",
    "\n",
    "map_200 = average_precision_score(sorted_class_matches[:, 0:limit], sorted_similarity[:, 0:limit])\n",
    "print(f'The mean average precision of the 200 most similar is {np.round(map_200, 4)}.')\n",
    "\n",
    "prec_200 = np.mean(sorted_class_matches[:, limit])\n",
    "print(f'The precision of the 200 most similar is {np.round(prec_200, 4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have all the necessary data to code the function to perform an inference on a sketch and retrieve the closest images ! \n",
    "\n",
    "The last part exercise of this notebook is to take part in coding the pipeline to plot images like the ones below:\n",
    "<img src=\"images/ant_and_attention.png\">\n",
    "<img src=\"images/ant_best_guess.png\">\n",
    "\n",
    "We break the problem into multiple steps in different cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute the embeddings and the attention map of the sketch\n",
    "sketch_embedding, sketch_attention = 0,0 #... # Implement me !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Solution\n",
    "sketch_embedding, sketch_attention = sk_net(sketch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_attention(attn, im):\n",
    "    \"\"\"\n",
    "    Gets the feature map of the attention and does a min-max normalisation (for further plots)\n",
    "    \"\"\"\n",
    "    attn = nn.Upsample(\n",
    "        size=(im[0].size(1), im[0].size(2)), mode=\"bilinear\", align_corners=False\n",
    "    )(attn)\n",
    "    min_attn = (\n",
    "        attn.view((attn.size(0), -1)).min(-1)[0]\n",
    "        .unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    )\n",
    "    max_attn = (\n",
    "        attn.view((attn.size(0), -1)).max(-1)[0]\n",
    "        .unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    )\n",
    "    return (attn - min_attn) / (max_attn - min_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attention values normalized and sized as the sketch\n",
    "sketch_attention = normalise_attention(sketch_attention, sketch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute similarity with image embeddings and sort index from \n",
    "similarity = ... # Implement me ! (Done before)\n",
    "arg_sorted_sim = ... # Implement me !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Solution\n",
    "similarity = get_similarity_solution(sketch_embedding, images_embeddings)\n",
    "similarity = similarity.squeeze()\n",
    "arg_sorted_sim = (-similarity).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Keep only the NUM_CLOSEST ones\n",
    "sorted_fnames = ...\n",
    "sorted_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Solution\n",
    "sorted_fnames = [images_fnames[i] for i in arg_sorted_sim[0: NUM_CLOSEST + 1]]\n",
    "sorted_labels = [images_classes[i] for i in arg_sorted_sim[0: NUM_CLOSEST + 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load images and get labels\n",
    "closest_images = [default_image_loader('../../' + fname) for fname in sorted_fnames]\n",
    "closest_labels = [dict_id_to_class[label] for label in sorted_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to plot the images. \n",
    "\n",
    "In the first image, we plot the original sketch on the left and then the superposed sketch with the attention heatmap on the right. \n",
    "\n",
    "In the second image, we plot the NUM_CLOSEST closest images with their labels in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot\n",
    "## Figure 1 ##\n",
    "fig1, ax1 = plt.subplots(1, 2, figsize=(NUM_CLOSEST*4, NUM_CLOSEST*2))\n",
    "\n",
    "# TODO: Plot sketch\n",
    "\n",
    "# TODO: Plot sketch with attention map\n",
    "heat_map = sketch_attention.squeeze() # Remove unecessary dimension from the heat map\n",
    "\n",
    "\n",
    "## Figure 2 ##\n",
    "fig2, ax2 = plt.subplots(1, NUM_CLOSEST, figsize=(NUM_CLOSEST*4,4))\n",
    "\n",
    "# TODO: Plot the NUM_CLOSEST images\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Solution\n",
    "fig1, ax1 = plt.subplots(1, 2, figsize=(NUM_CLOSEST*4,NUM_CLOSEST*2))\n",
    "\n",
    "## Figure 1 ##\n",
    "# Plot sketch and attention map\n",
    "sk = plt.imread(sketch_fname)\n",
    "ax1[0].imshow(sk, cmap=\"gray\")\n",
    "ax1[0].set(title=\"Sketch \\n Label: \" + sketch_fname.split(\"/\")[-2])\n",
    "ax1[0].axis(\"off\")\n",
    "\n",
    "# We remove unecessary dimension from the heat map for the plot\n",
    "heat_map = sketch_attention.squeeze()\n",
    "ax1[1].imshow(sk)\n",
    "ax1[1].imshow(255 * heat_map, alpha=0.7, cmap=\"Spectral_r\")\n",
    "ax1[1].set(title=sketch_fname.split(\"/\")[-2] + \"\\n Attention Map\")\n",
    "ax1[1].axis(\"off\")\n",
    "\n",
    "## Figure 2 ##\n",
    "fig2, ax2 = plt.subplots(1, NUM_CLOSEST, figsize=(NUM_CLOSEST*4,4))\n",
    "for i in range(NUM_CLOSEST):\n",
    "    ax2[i].imshow(closest_images[i])\n",
    "    ax2[i].set(title='Closest image ' + str(i) + '\\n Label: ' + closest_labels[i])\n",
    "    ax2[i].axis('off')\n",
    "\n",
    "#plt.subplots_adjust(wspace=0.25, hspace=-0.35)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amld",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
