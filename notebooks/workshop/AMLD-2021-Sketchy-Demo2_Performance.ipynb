{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLD-2021-Sketchy: Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we visualised the datasets, coded the model encoder and the losses. In this notebook, we compute the perfomance of the model and then visualise the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the autoreload package into ipython kernel and set it to automatically reload modules\n",
    "# when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Import the code and install dependencies...')\n",
    "    !git clone https://github.com/VisiumCH/AMLD-2021-Sketchy.git AMLD-2021-Sketchy\n",
    "    %cd AMLD-2021-Sketchy/\n",
    "    !git checkout workshop_notebook\n",
    "    !pip install -e .\n",
    "    \n",
    "    !pip install googledrivedownloader\n",
    "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "    \n",
    "    gdd.download_file_from_google_drive(file_id='1ccHudkezwSFFXX5q5Pvh8A37LDrxrzDC',\n",
    "                                    dest_path='./QuickdrawSubset.zip',\n",
    "                                    unzip=True)\n",
    "    gdd.download_file_from_google_drive(file_id='1YQJ8TPFlmMZhKiRaZ2oB18edKKORSzjN',\n",
    "                                    dest_path='./precomputed_embeddings.zip',\n",
    "                                    unzip=True)\n",
    "    gdd.download_file_from_google_drive(file_id='1fQUbRhurMWrJg6zewPGYcoHmpOz3mJgC',\n",
    "                                    dest_path='./checkpoint.pth')\n",
    "    gdd.download_file_from_google_drive(file_id='1EVRo2q0utNjRqOJbezmn-UPQ2kipgopa',\n",
    "                                    dest_path='./lion_sketch.png')\n",
    "    \n",
    "    \n",
    "    DATA_PATH =  'QuickdrawSubset'\n",
    "    CHECKPOINT_PATH = \"checkpoint.pth\"\n",
    "    MODEL_PATH = 'precomputed_embeddings'\n",
    "    SKETCH_FNAME = 'lion_sketch.png'\n",
    "    \n",
    "else:\n",
    "    CHECKPOINT_PATH = \"/io/models/quickdraw_training/checkpoint.pth\"\n",
    "    MODEL_PATH = \"/io/models/quickdraw_training/precomputed_embeddings\"\n",
    "    DATA_PATH =  \"/io/data/raw/Quickdraw\"\n",
    "    SKETCH_FNAME = 'images/lion_sketch.png'\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data.utils import default_image_loader, get_class_dict\n",
    "from src.models.encoder import EncoderCNN\n",
    "from src.models.utils import load_checkpoint, normalise_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we describe how to compute the performance metrics. We present an overall explanation and then multiple exercise will guide to computation.\n",
    "\n",
    "The metrics that we aim to compute are `map_`, `map_100`, and `prec_100`. These metrics are all computed based on the embeddings. \n",
    "\n",
    "For each sketch embedding, we order the images embeddings based on their 'similarity'. The closer the image is to the sketch, the more 'similar' it is. \n",
    "\n",
    "Therefore, we create a `similarity` matrix and a `class_matches` matrix. The matrices have a row for each sketch and a column for each image. \n",
    "\n",
    "In the `similarity` matrix, each cell correspond to the distance between the sketch and the image. In the `class_matches` matrix, each cell is one if the class of the sketch and the image are the same and 0 otherwise.\n",
    "\n",
    "We then order the columns of `similarity` and `class_matches` based on the similarity values: the columns are reordered such that the most similar columns are first and the least similar are last. The sorted matrices are `sorted_similarity` and `sorted_class_match` respectively.\n",
    "\n",
    "Once we have the `sorted_similarity` and `sorted_class_matches` matrices, we can compute the metrics easily. \n",
    "\n",
    "- `map_` is the [mean average precision](https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2) of all the items. It summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight: $AP=\\sum_n(R_n - R_{n-1})P_n$ where $P_n$ and $R_n$ are the precision and recall at the nth threshold. (See [sklearn.metrics.average_precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html).)\n",
    "\n",
    "- `map_100` is the mean average precision of the 100 most similar items to each sketch (100 first columns).\n",
    "\n",
    "- `prec_100` is the precision of the 100 first most similar items to each sketch (100 first columns). For one sketch, it is the sum of classes that matched (with a 1) divided by the number of images (100). For all sketches, it is the mean of the result for each sketch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images and sketch embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained, to compute the metrics, we need the embeddings of each images and sketches of the **test set**. Therefore, we need to iterate through all images and sketches and pass them through the image or sketch encoder respectively to get the embedding.\n",
    "\n",
    "As this is a long and quite straightforward process, we already have done it for you and saved them with their labels. Here is the function to load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precomputed_embeddings(embeddings_path, image_type):\n",
    "    \n",
    "    emb_path = os.path.join(embeddings_path, f\"quickdraw_{image_type}_array.npy\")\n",
    "    metadata_path = os.path.join(embeddings_path, f\"quickdraw_{image_type}_meta.csv\")\n",
    "\n",
    "    with open(emb_path, \"rb\") as f:\n",
    "        images_embeddings = np.load(f)\n",
    "        \n",
    "    df = pd.read_csv(metadata_path, sep=\" \")\n",
    "    images_fnames, images_classes = df[\"fnames\"].to_list(), df[\"classes\"].to_list()\n",
    "    \n",
    "    return images_embeddings, images_classes, images_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_embeddings, images_classes, images_fnames = load_precomputed_embeddings(MODEL_PATH, 'images')\n",
    "sketch_embeddings, sketch_classes, sketch_fnames = load_precomputed_embeddings(MODEL_PATH, 'sketches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_embeddings.shape, sketch_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both the embeddings for the images and the sketches, we compute the similarity between them in the embeddings space. We define the similarity by the following simple function (with the distance being the Euclidian distance between both embeddings): $$similarity = \\frac{1}{1 + distance}$$ Embeddings close to each others are more similar than embeddings far from each others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Finish to code the `get_similarity` function ([cdist documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(image_embeddings, sketch_embeddings):\n",
    "    '''\n",
    "    Computes images and sketch similarity in the feature space.\n",
    "    The distance is computed as the euclidean distance. \n",
    "    Here, we want similarity = 1/(1 + distance)\n",
    "    Args:\n",
    "        - im_embeddings: embeddings of the images [MxE]\n",
    "        - sketch_embeddings: embeddings of the sketches [NxE]\n",
    "    Return:\n",
    "        - similarity: similarity value between images and sketches embeddings [NxM]\n",
    "    '''\n",
    "    \n",
    "    similarity = ... # Implement me !\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the `class_matches` matrix (the rows are sketches and the columns are images) with 1 where the label and sketch match and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Finish to code the `compare_classes` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classes(class_image, class_sketch):\n",
    "    '''\n",
    "    Compare classes of images and sketches\n",
    "    Args:\n",
    "        - class_im: list of classes of the images [M]\n",
    "        - class_sk: list of classes of the sketches [N]\n",
    "    Return:\n",
    "        - array [NxM] of 1 where the image and sketch belong to the same class and 0 elsewhere\n",
    "    '''\n",
    "    class_sketch = np.expand_dims(class_sketch, axis=1)\n",
    "    class_image = np.expand_dims(class_image, axis=0)\n",
    "    class_matches = ... # Implement me!\n",
    "    return class_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_matches = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now sort the matrices by similarity meaning that for each sketch, we reorder the columns based on their similarity values. An image similar to a sketch should be in the beginning of the list (first columns) and a very different image should be at the end of the list (last columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_class_by_similarity(similarity, class_matches):\n",
    "    '''\n",
    "    Sort the matrices by similarity. Column order based on similarity.\n",
    "    The distance is computed as the euclidean distance. \n",
    "    Args:\n",
    "        - similarity: array [NxM] similarity between sketches and images\n",
    "        - class_matches: array [NxM], 1 where the image and sketch belong to the same class, 0 elsewhere\n",
    "    Return:\n",
    "        - sorted_similarity: array [NxM] sorted similarity between sketches and images\n",
    "        - sorted_class_matches: array [NxM], sorted class_matches\n",
    "    '''\n",
    "    similarity.squeeze()\n",
    "    sorted_similarity_indexes = (-similarity).argsort()\n",
    "    sorted_similarity = []  # list of similarity values ordered by similarity (most to least similar)\n",
    "    sorted_class = []       # list of class match ordered by similarity (0 if different, 1 if same)\n",
    "    for index in range(0, sorted_similarity_indexes.shape[0]):\n",
    "        sorted_similarity.append(similarity[index, sorted_similarity_indexes[index, :]])\n",
    "        sorted_class.append(class_matches[index, sorted_similarity_indexes[index, :]])\n",
    "\n",
    "    sorted_similarity = np.array(sorted_similarity)\n",
    "    sorted_class_matches = np.array(sorted_class)\n",
    "    return sorted_similarity, sorted_class_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_similarity, sorted_class_matches = sort_class_by_similarity(similarity, class_matches)\n",
    "sorted_similarity.shape, sorted_class_matches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Compute the mAP (mean average precision), mAP@100 and Precision@100 of the results. ([average_precision_score documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean average precision of all results\n",
    "# Hint: you can use sklearn function average_precision_score\n",
    "map_ = ... # Implement me!\n",
    "\n",
    "limit = 100\n",
    "# Compute the mean average precision of the 100 first results\n",
    "map_100 = ... # Implement me!\n",
    "\n",
    "# Compute the Precision@100 (meaning the precision at the place 100th)\n",
    "prec_100 =... # Implement me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform an inference on a sketch, retrieve the NUM_CLOSEST closest images and plot them together.\n",
    "\n",
    "The last part exercise of this notebook is to take part in coding the pipeline to plot images of attention and inference of a sketch.\n",
    "We break the problem into multiple steps in different cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLOSEST = 4 # number of closest images to show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The images embeddings were already loaded in the `Metric Computation` part and are ready to use.\n",
    "\n",
    "We need a sketch to perform the inference on. Therefore, we prepared the image of the camel below. We load it, prepare it to go through the sketch encoder network and plot it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the camel sketch\n",
    "sketch = default_image_loader(SKETCH_FNAME) # basically load and resize the image\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "sketch = transform(sketch) # apply transform\n",
    "sketch = sketch.unsqueeze(0) # expand because no batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small sketch plot\n",
    "plt.imshow(plt.imread(SKETCH_FNAME), cmap=\"gray\");\n",
    "plt.title('Sketch Example: apple')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "We need a model to transform the sketch to its embedding. Therefore, we load the encoder network with the sketch checkpoint data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model\n",
    "EMBEDDING_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Are you able to load the model of sketch encoder ? We did it in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the encoder, load the checkpoint and then load the checkpoint in the sketch encoder\n",
    "sketch_encoder = ...\n",
    "checkpoint = ...\n",
    "sketch_encoder.load_state_dict(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_encoder.eval()\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "We now have everything to do the inference! Let's get on :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Compute the embeddings and the attention map of the sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the embeddings and the attention map of the sketch\n",
    "sketch_embedding, sketch_attention = 0,0 #... # Implement me !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the plot, we resize the attention such that it is the same size as the initial sketch and normalise it between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get the attention values normalized and at the same size as the sketch\n",
    "sketch_attention = normalise_attention(sketch_attention, sketch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Compute now the similarity between images and sketch and sort the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute similarity with image embeddings and sort index from \n",
    "similarity = ... # Implement me !\n",
    "sorted_similarity_indexes = ... # Implement me !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Keep the labels of the NUM_CLOSEST most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Keep only the NUM_CLOSEST ones\n",
    "sorted_fnames = ...\n",
    "sorted_labels = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the image and retrieve the class names based on the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load images and get labels\n",
    "dict_class_to_id = get_class_dict(DATA_PATH)\n",
    "dict_id_to_class = {v:k for k,v in dict_class_to_id.items()}\n",
    "closest_labels = [dict_id_to_class[label] for label in sorted_labels]\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    closest_images = [default_image_loader(DATA_PATH + '/' + '/'.join(fname.split('/')[-3:])) for fname in sorted_fnames]\n",
    "else:\n",
    "    closest_images = [default_image_loader('../../' + fname) for fname in sorted_fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to plot the images. \n",
    "\n",
    "In the first image, we plot the original sketch on the left and then the superposed sketch with the attention heatmap on the right. \n",
    "\n",
    "**TODO**: In the second image, plot the NUM_CLOSEST closest images with their labels in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot\n",
    "## Figure 1 ##\n",
    "fig1, ax1 = plt.subplots(1, 2, figsize=(NUM_CLOSEST*4, NUM_CLOSEST*2))\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    name = SKETCH_FNAME\n",
    "else:\n",
    "    name = SKETCH_FNAME.split(\"/\")[-2]\n",
    "\n",
    "# Plot sketch and attention map\n",
    "sk = plt.imread(SKETCH_FNAME)\n",
    "ax1[0].imshow(sk, cmap=\"gray\")\n",
    "ax1[0].set(title=\"Sketch \\n Label: \" + name)\n",
    "ax1[0].axis(\"off\")\n",
    "\n",
    "# We remove unecessary dimension from the heat map for the plot\n",
    "heat_map = sketch_attention.squeeze() # Remove unecessary dimension from the heat map\n",
    "ax1[1].imshow(sk)\n",
    "ax1[1].imshow(255 * heat_map, alpha=0.7, cmap=\"Spectral_r\")\n",
    "ax1[1].set(title=name + \"\\n Attention Map\")\n",
    "ax1[1].axis(\"off\")\n",
    "\n",
    "## Figure 2 ##\n",
    "fig2, ax2 = plt.subplots(1, NUM_CLOSEST, figsize=(NUM_CLOSEST*4,4))\n",
    "for i in range(NUM_CLOSEST):\n",
    "    # TODO: Plot the i_th images with its label\n",
    "    ax2[i].imshow([[0], [0]]) # Replace with image\n",
    "    ax2[i].set(title='Closest image ' + str(i) + '\\n Label: ' + '') # Add class name\n",
    "    ax2[i].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done, congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amld",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
