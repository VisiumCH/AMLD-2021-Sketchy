{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: This notebook provides a walk through the process of retrieving photos of the same class as a hand-drawn sketch with the model proposed in [Doodle to Search: Practical Zero-Shot Sketch-based Image Retrieval](http://dagapp.cvc.uab.es/doodle2search/CVPR2019.pdf). \n",
    "Complementary information and studies on the model can be found in [here](http://dagapp.cvc.uab.es/doodle2search/CVPR2019_Supplementery.pdf).\n",
    "\n",
    "Contrarily to the paper above, we perform here a non-zero shot inference, which means that the classes in training, validation and testing are the same (only sketches and images are different). Moreover, we did not implement all the parts associated with the semantic loss. Further explanation are èrovided in the model description part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pauline.maury/AMLD-2021-Sketchy/notebooks/workshop'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!git clone https://https://github.com/VisiumCH/AMLD-2021-Sketchy.git\n",
    "\n",
    "#%cd AMLD-2021-Sketchy/notebooks/\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Codebase imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick overview\n",
    "\n",
    "The photo retrieval task aims to return photos that are in the same class as sketches drawn by hand.\n",
    "\n",
    "Therefore, two encoders are trained: one for the photos and one for the sketches. Each encoder maps its input (photo or sketch) to an embedding space E (with E=256 dimensions).\n",
    "\n",
    "Then, the embeddings are ranked by similarity based on their euclidean distance in the embedding space and the most similar photos to the sketch  are retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "$C$: set of all possible categories\n",
    "\n",
    "$X = \\{x_i\\}^N_{i=1}$: set of photos\n",
    "\n",
    "$Y = \\{y_i\\}^M_{i=1}$: set of sketches\n",
    "\n",
    "$l_x : X → C$ and $l_y : Y → C$ are the two labelling functions for photos and sketches respectively.\n",
    "\n",
    "Such that given an input sketch an optimal ranking of gallery images can be obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Networks\n",
    "Given a distance function d(·, ·), the aim of the framework is to learn two embedding functions $Ф : X → R^D$ and $Ψ : Y → R^D$, which respectively map the photo and sketch domain into a common embedding space.\n",
    "\n",
    "Given two photos $x_1, x_2 ∈ X$ and a sketch $y ∈ Y$, it is expected that the embeddings fulfill the following condition:\n",
    "\n",
    "$$\\begin{align}\n",
    "& d(Ф(x_1), Ψ(y)) < d(Ф(x2), Ψ(y)), \\\\\n",
    "& when \\quad l_x(x_1) = l_y(y) \\\\\n",
    "& and \\quad l_x(x_2) ≠ l_y(y) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "meaning that there is a shorter distance when photos and sketch belong to the same class. Here, $d$ is the euclidean distance.\n",
    "\n",
    "The embedding function $Ф(·)$ and $Ψ(·)$ are defined as two CNNs (VGG 16) with attention where the last fully-connected layer has been replaced to match the embedding size E. The attention mechanism helps the system\n",
    "to localise the important features and is learned end-to-end with the rest of the network. The output of the attention module is computed by $f + f * att$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning objective of the framework combines two losses: the <i>Triplet Loss</i> and the <i>Domain Loss</i>.\n",
    "\n",
    "$\\{a, p, n\\}$ \n",
    "where $a ∈ Y^s$, $p ∈ X^s$ and $n ∈ X^s$ are respectively the anchor, positive and negative samples during\n",
    "the training and $l_x(p) = l_y(a)$ and $l_x(n) ≠ l_y(a)$.\n",
    "\n",
    "#### Triplet loss: \n",
    "This loss aims to reduce the distance between embedded sketch and image if they belong to the same class and increase it if they belong to different classes.\n",
    "\n",
    "Defining distance between samples as $δ_+ = ||Ψ(a) − Ф(p)||_2$ and $δ_- = ||Ψ(a) − Ф(n)||_2$ for the positive and negative samples respectively, then, the ranking loss for a particular triplet can be formulated as $λ(δ_+, δ_−) = max\\{0, µ+δ_+ −δ_−\\}$ where $µ > 0$ is a margin parameter. \n",
    "\n",
    "Batch-wise, the loss is defined as:\n",
    "$$\\begin{align}\n",
    "& L_t = \\sum_{i=1}^{N} λ(δ^i_+, δ^i_-) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    " \n",
    "the order aimed by this loss is $δ_− > δ_+ + µ$, if this is the case, the network is not updated, otherwise, the weights of the network are updated accordingly.\n",
    "\n",
    "#### Domain loss:\n",
    "This loss aims to explicitly enforce the mapping of sketch and image samples to a common space.\n",
    "\n",
    "Given the embedding $Ф(·)$ and $Ψ(·)$, we make use of a Multilayer Perceptron (MLP) as a binary classifier trying to predict which was the initial domain. Purposefully, in order to create indistinguishable embedding we use a GRL defined as $R_λ(·)$, which applies the identity function during the forward pass $R_λ(x) = x$,\n",
    "whereas during the backward pass it multiplies the gradients by the meta-parameter $−λ$, $\\frac{dR_λ}{dx}= −λI$. This operation reverses the sign of the gradient that flows through the CNNs. \n",
    "\n",
    "Following the notation,\n",
    "$f : R^D → [0, 1]$ be the MLP and $e ∈ R^D$ an embedding coming from the encoders network. Then we can define the binary cross entropy of one of the samples as $l_t(e) = tlog(f(R_{λ_d}(e))) + (1 − t) log(1 − f(R_{λ_d}(e)))$, where $e$ is the embedding obtained by the encoder network and $t$ is 0 and 1 for sketch and photo domains respectively.\n",
    "Hence, the domain loss is defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "& L_d = \\frac{1}{3N}\\sum_{i=1}^{N} ( l_0(Ψ(a_i)) + l_1(Ф(p_i)) + l_1(Ф(n_i)) ) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "See the paper for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMLD env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
