{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLD-2021-Sketchy: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: This notebook provides a walk through the process of retrieving photos of the same class as a hand-drawn sketch with the model proposed in [Doodle to Search: Practical Zero-Shot Sketch-based Image Retrieval](http://dagapp.cvc.uab.es/doodle2search/CVPR2019.pdf). \n",
    "Complementary information and studies on the model can be found in [here](http://dagapp.cvc.uab.es/doodle2search/CVPR2019_Supplementery.pdf).\n",
    "\n",
    "Contrarily to the paper above, we perform here a non-zero shot inference, which means that the classes in training, validation and testing are the same (only sketches and images are different). Moreover, we did not implement the semantic loss. Further explanation are provided in the model description part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the autoreload package into ipython kernel and set it to automatically reload modules\n",
    "# when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    \n",
    "    print('Import the code and install dependencies...')\n",
    "    !git clone https://github.com/VisiumCH/AMLD-2021-Sketchy.git AMLD-2021-Sketchy\n",
    "    %cd AMLD-2021-Sketchy/\n",
    "    !git checkout workshop_notebook\n",
    "    !pip install -e .\n",
    "    \n",
    "    !pip install googledrivedownloader\n",
    "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "    \n",
    "    gdd.download_file_from_google_drive(file_id='1ccHudkezwSFFXX5q5Pvh8A37LDrxrzDC',\n",
    "                                    dest_path='./QuickdrawSubset.zip',\n",
    "                                    unzip=True)\n",
    "    gdd.download_file_from_google_drive(file_id='1fQUbRhurMWrJg6zewPGYcoHmpOz3mJgC',\n",
    "                                    dest_path='./checkpoint.pth')\n",
    "    \n",
    "    DATA_PATH =  'QuickdrawSubset'\n",
    "    CHECKPOINT_PATH = \"checkpoint.pth\"\n",
    "    \n",
    "else:\n",
    "    CHECKPOINT_PATH = \"/io/models/quickdraw_training/checkpoint.pth\"\n",
    "    DATA_PATH =  \"/io/data/raw/Quickdraw\"\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as vision_models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Codebase imports\n",
    "from src.data.default_dataset import DefaultDataset\n",
    "from src.data.utils import default_image_loader, get_class_dict, dataset_split\n",
    "from src.models.loss import GradReverse\n",
    "from src.models.utils import load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick overview\n",
    "\n",
    "The sketch-based photo retrieval task aims to return photos that are in the same class as sketches drawn by hand. Therefore, two encoders are trained: one for photos and one for sketches. Each encoder maps its input (photo or sketch) to an embedding space E. Then, the embeddings are ranked by similarity based on their euclidean distance in the embedding space and the most similar photos to the sketch  are retrieved.\n",
    "\n",
    "![img](https://drive.google.com/uc?export=view&id=1UP6VT5BvKvW_ur2NzK1DTPQ0srkf2uFH)\n",
    "<center> Figure 1: Sketch and Images Embeddings distance </center>\n",
    "\n",
    "In the figure 1, the sketch is an elephant, image 1 is also an elephant and image 2 is a mountain. The image encoder computes the embeddings $E_{i1}$ and $E_{i2}$ from the image 1 and image 2 respectively. The sketch encoder computes the embedding $E_s$.\n",
    "\n",
    "In the next steps, the **sketch $E_s$ is the anchor**, **$E_{i1}$ is the positive sample** as it belongs to the same class and **$E_{i2}$ is the negative sample** as it belongs to another class.\n",
    "\n",
    "As $E_{i1}$ and $E_s$ belong to the same class (elephant) and $E_{i2}$ belongs to another class (mountain), the encoders should compute the embeddings of $E_{i1}$ and $E_s$ 'close' to each other and far from $E_{i2}$.\n",
    "\n",
    "When $E_s$ is drawn, $E_{i1}$ is closer to it than $E_{i2}$ so we retrieve the image 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first visualise some random data triplets from the dataset with one sketch, positive image (same class as the sketch) and one negative images (different class than the sketch) as they would typically be fed to the network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset classes\n",
    "dict_class_to_id = get_class_dict(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and test data\n",
    "TRAINING_SPLIT = 0.8\n",
    "VALID_SPLIT = 0.1\n",
    "train_data, _, _ = dataset_split(DATA_PATH, TRAINING_SPLIT, VALID_SPLIT)\n",
    "\n",
    "train_data = DefaultDataset(\n",
    "    mode=\"train\", \n",
    "    data=train_data, \n",
    "    dataset_folder=DATA_PATH, \n",
    "    dicts_class=dict_class_to_id, \n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's now load and visualize some training data\n",
    "BATCH_SIZE = 10\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# We will work on a single batch\n",
    "sketchs, positive_images, negative_images, positive_labels, negative_labels = next(iter(train_loader))\n",
    "positive_labels, negative_labels = positive_labels.numpy(), negative_labels.numpy()\n",
    "\n",
    "# Reverse dict to go from id to class\n",
    "dict_id_to_class = {v:k for k,v in dict_class_to_id.items()}\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18,6))\n",
    "    plt.suptitle('Left to right: Sketch, Positive image, Negative image', fontsize=20)\n",
    "    ax[0].imshow(sketchs[i].permute(1,2,0).numpy())\n",
    "    ax[0].set_title(dict_id_to_class[positive_labels[i]], fontsize=17)\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(positive_images[i].permute(1,2,0).numpy())\n",
    "    ax[1].set_title(dict_id_to_class[positive_labels[i]], fontsize=17)\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    ax[2].imshow(negative_images[i].permute(1,2,0).numpy())\n",
    "    ax[2].set_title(dict_id_to_class[negative_labels[i]], fontsize=17)\n",
    "    ax[2].axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We now focus on the model training in two main parts: coding the model network architecture and coding the loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formulation\n",
    "\n",
    "We have:\n",
    "\n",
    "$$\\begin{align}\n",
    "& X = \\{x_i\\}^N_{i=1} \n",
    "& l_x : X → C \\\\\n",
    "& Y = \\{y_i\\}^M_{i=1} \n",
    "& l_y : Y → C \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "with $X$ the set of photos, $Y$ the set of sketches, $C$ the set of all possible categories and $l_x$ and $l_y$ the two labelling functions for photos and sketches respectively.\n",
    "\n",
    "## Encoder Networks\n",
    "Given a distance function d(·, ·), the aim of the framework is to learn two embedding functions $Ф : X → R^D$ and $Ψ : Y → R^D$, which respectively map the photo and sketch domain into a close together in a common embedding space.\n",
    "\n",
    "Given two photos $x_1, x_2 ∈ X$ of class C1 and C2 and a sketch $y ∈ Y$ of class C1, we want the embedding of $x_1$ to be closer to the one of $y$ than the one of $x_1$. Mathematically, it is expected that the embeddings fulfill the following condition:\n",
    "\n",
    "$$\\begin{align}\n",
    "& d(Ф(x_1), Ψ(y)) < d(Ф(x2), Ψ(y)), \\\\\n",
    "& when \\quad l_x(x_1) = l_y(y) \\quad and \\quad l_x(x_2) ≠ l_y(y) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "meaning that there is a shorter distance when photos and sketch belong to the same class, than when they don't. Here, $d$ is the euclidean distance.\n",
    "\n",
    "In our case, the embedding functions $Ф(·)$ and $Ψ(·)$ are defined as two CNNs with attention: VGG 16 networks with additional attention layers and where the last fully-connected layer has been replaced to match the embedding size E. The attention mechanism helps the system\n",
    "to localise the important features and is learned end-to-end with the rest of the network. The output of the attention module is computed by $f + f * att$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://drive.google.com/uc?export=view&id=1O0bL39vhWlptY8rVVseWOvEQAwBJu9Dk)\n",
    "<center> Figure 3: Encoder architecture </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention module to give more weights to important areas\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, hidden_layer=380):\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        self.attn_hidden_layer = hidden_layer\n",
    "        self.net = nn.Sequential(nn.Conv2d(512, self.attn_hidden_layer, kernel_size=1),\n",
    "                                 nn.Conv2d(self.attn_hidden_layer, 1, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_mask = self.net(x)\n",
    "        attn_mask = attn_mask.view(attn_mask.size(0), -1)\n",
    "        attn_mask = nn.Softmax(dim=1)(attn_mask)\n",
    "        attn_mask = attn_mask.view(attn_mask.size(0), 1, x.size(2), x.size(3))\n",
    "        x_attn = x * attn_mask\n",
    "        x = x + x_attn\n",
    "        return x, attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the encoder following the diagram below. The last layer of the VGG-16 classifier is replaced and outputs the embeddings.\n",
    "At the end of the forward pass, the output of the network and the attention are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: [vgg16_bn](https://pytorch.org/vision/stable/models.html#torchvision.models.vgg16_bn) is a pretrained VGG 16 network from pytorch. It is instantiated with:\n",
    "- vgg_model = vision_models.vgg16_bn(pretrained)\n",
    "\n",
    "and can then be split in different parts:\n",
    "- vgg_model.features for the CNN features extraction part of the network\n",
    "- vgg_model.classifier for the MLP classier part of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embedding_size=256, pretrained=True):\n",
    "        super(EncoderCNN, self).__init__() \n",
    "        \n",
    "        # VGG-16 features\n",
    "        vgg_model = vision_models.vgg16_bn(pretrained)\n",
    "        self.cnn_features = ...\n",
    "\n",
    "        # Attention module\n",
    "        self.attn = ...\n",
    "        \n",
    "        # Classifier\n",
    "        self.map = ...\n",
    "        self.map._modules['6'] = nn.Linear(4096, embedding_size)\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = ...\n",
    "        return x, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model\n",
    "EMBEDDING_SIZE = 256\n",
    "image_encoder = ...\n",
    "sketch_encoder = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder.train()\n",
    "sketch_encoder.train()\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "checkpoint = load_checkpoint(CHECKPOINT_PATH)\n",
    "image_encoder.load_state_dict(checkpoint[\"im_state\"], strict=False)\n",
    "sketch_encoder.load_state_dict(checkpoint[\"sk_state\"], strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "Now that the encoder is ready, we code the different losses required to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the losses, let's first get the embeddings of the images from the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_image_embeddings, _ = image_encoder(positive_images)\n",
    "negative_image_embeddings, _ = image_encoder(negative_images)\n",
    "sketch_embeddings, _ = sketch_encoder(sketchs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning objective of the framework combines two losses: the <i>Triplet Margin Loss</i> and the <i>Domain Loss</i>.\n",
    "\n",
    "Let $\\{a, p, n\\}$, where $a ∈ Y$, $p ∈ X$ and $n ∈ X$ be respectively the anchor, positive and negative samples during the training and $l_x(p) = l_y(a)$ and $l_x(n) ≠ l_y(a)$ as we have seen in the data at the beginning of the notebook.\n",
    "\n",
    "\n",
    "\n",
    "Figure 2 shows a graphical example of which parameters the losses are taking. See the paper for additional information.\n",
    "![img](https://drive.google.com/uc?export=view&id=10riX6BvLlV5dG2KZsFQJu7ekKi08szv1)\n",
    "<center> Figure 2: Losses in the model training </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet margin loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss aims to reduce the distance between embedded sketch and image if they belong to the same class and increase it if they belong to different classes.\n",
    "\n",
    "Defining distance between samples as $δ_+ = ||Ψ(a) − Ф(p)||_2$ and $δ_- = ||Ψ(a) − Ф(n)||_2$ for the positive and negative samples respectively, then, the ranking loss for a particular triplet can be formulated as $λ(δ_+, δ_−) = max\\{0, µ+δ_+ −δ_−\\}$ where $µ > 0$ is a margin parameter. \n",
    "\n",
    "\n",
    "Batch-wise, the loss is defined as:\n",
    "$$\\begin{align}\n",
    "& L_t = \\frac{1}{N}\\sum_{i=1}^{N} λ(δ^i_+, δ^i_-) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    " \n",
    "the order aimed by this loss is $δ_− > δ_+ + µ$, if this is the case, the network is not updated, otherwise, the weights of the network are updated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the triplet margin loss step by step based on the formulas provided in the Learning Objectives' Triplet Margin Loss.\n",
    "\n",
    "*N.B*: This loss is implemented in PyTorch as nn.TripletMarginLoss but we code it here from scratch. \n",
    "\n",
    "**Hint**: for the deltas, you can use [torch.linalg.norm](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIPLET_MARGIN = 1\n",
    "\n",
    "def calculate_triplet_margin_loss(sketch_embeddings, positive_image_embeddings, negative_image_embeddings):\n",
    "    \n",
    "    triplet_margin_loss = ... # Implement me !\n",
    "    return triplet_margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_margin_loss = calculate_triplet_margin_loss(sketch_embeddings, positive_image_embeddings, negative_image_embeddings)\n",
    "triplet_margin_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Loss\n",
    "\n",
    "This loss aims to explicitly enforce the mapping of sketch and image samples to a common space.\n",
    "\n",
    "Given the embedding $Ф(·)$ and $Ψ(·)$, we make use of a Multilayer Perceptron (MLP) as a binary classifier $f(·)$ trying to predict which was the initial domain. \n",
    "\n",
    "Purposefully, in order to create indistinguishable embedding we use a Gradient Reversal Layer (GRL) defined as $R_{λ_d}(·)$, which applies the identity function during the forward pass $R_{λ_d}(x) = x$,\n",
    "whereas during the backward pass it multiplies the gradients by the meta-parameter $−λ_d$, such that the gradient $\\frac{dR_{λ_d}}{dx}= −{λ_d}I$. \n",
    "\n",
    "This operation reverses the sign of the gradient that flows through the CNN Encoders, so that we train them to produce embeddings indistinguishable from each other for the MLP.\n",
    "\n",
    "During the training, the meta-parameter $λ_d$ changes from 0 (only trains the classifier but does not update the encoder network) to 1 during the training. \n",
    "Here, before the 5th epoch $λ_d=0$, after the 25th $λ_d=1$, and in-between $λ_d(epoch) = (epoch − 5)/20$\n",
    "\n",
    "With $f : R^D → [0, 1]$ as the MLP function and $e ∈ R^D$ as an embedding coming from the encoders network. We can define the binary cross entropy of one of the samples as $l_t(e) = tlog(f(R_{λ_d}(e))) + (1 − t) log(1 − f(R_{λ_d}(e)))$, where $e$ is the embedding obtained by the encoder network and $t$ is the target label, 0 for sketch and 1 for photo. Hence, the domain loss is defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "& L_d = \\frac{1}{3N}\\sum_{i=1}^{N} ( l_0(Ψ(a_i)) + l_1(Ф(p_i)) + l_1(Ф(n_i)) ) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Here, we already have implemented the reverse gradient and the loss from the binary classifier (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the domain loss (see Learning Objectives: Domain Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_reverse(x, lambd=0.5):\n",
    "    '''\n",
    "    Forward pass R(x) = x\n",
    "    Backward pass R(X) = -λx\n",
    "    '''\n",
    "    return GradReverse.apply(x, lambd)\n",
    "\n",
    "class DomainLoss(nn.Module):\n",
    "    '''Ensures that embeddings belong to the same space'''\n",
    "\n",
    "    def __init__(self, input_size=256, hidden_size=64):\n",
    "        super(DomainLoss, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        # self.map = nn.Linear(self.input_size, 1)\n",
    "        self.map = nn.Sequential(\n",
    "            nn.Linear(self.input_size, hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = self.map(x)\n",
    "        x = torch.sigmoid(x).squeeze()\n",
    "        return F.binary_cross_entropy(x, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: useful for get_lambda_d function [np.clip](https://numpy.org/doc/stable/reference/generated/numpy.clip.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda_d(epoch):\n",
    "    lambda_d = ... # Implement me!\n",
    "    return lambda_d\n",
    "\n",
    "def calculate_domain_loss(sketch_embeddings, positive_image_embeddings, negative_image_embeddings, epoch=10):\n",
    "    # Get Lambda\n",
    "    lambda_d = get_lambda_d(epoch)\n",
    "    \n",
    "    # Outputs the binary cross entropy between the input and the target\n",
    "    domain_loss = DomainLoss(input_size=EMBEDDING_SIZE)\n",
    "\n",
    "    # Hint: 1. create the target for the classification between sketch and images. (tensor of size BATCH_SIZE)\n",
    "    #       2. add the backward pass of the binary crossentropy between inputs and targets.\n",
    "    \n",
    "    loss_dom = ... # Implement me !\n",
    "    return loss_dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_loss = calculate_domain_loss(sketch_embeddings, positive_image_embeddings, negative_image_embeddings)\n",
    "domain_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss is weighted average of the triplet margin loss and the domain loss.\n",
    "$$\\begin{align}\n",
    "& L_{tot} = w_d*L_d+w_t*L_t\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: And now get the final loss weighting the domain and the triplet margin loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_loss(\n",
    "    sketch_embeddings,\n",
    "    positive_image_embeddings,\n",
    "    negative_image_embeddings,\n",
    "    epoch = 10,\n",
    "    w_triplet = 1,\n",
    "    w_domain = 1\n",
    "):\n",
    "    \n",
    "    loss = ... # Implement me !\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = calculate_final_loss(sketch_embeddings, positive_image_embeddings, negative_image_embeddings)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercise: model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check that our final loss is lower on the trained model than a raw model with no pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_loss = ...\n",
    "print(f\"Loss on the trained model: {trained_loss}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute loss with an untrained model for comparison (the model is not trained if you don't load weights!)\n",
    "image_encoder_untrained = ...\n",
    "sketch_encoder_untrained = ...\n",
    "\n",
    "positive_image_embeddings, _ = ...\n",
    "negative_image_embeddings, _ = ...\n",
    "sketch_embeddings, _ = ...\n",
    "\n",
    "untrained_loss = ...\n",
    "print(f\"Loss on the untrained model: {untrained_loss}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss of the trained model should be lower than the one of the untrained model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (trained_loss < untrained_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
